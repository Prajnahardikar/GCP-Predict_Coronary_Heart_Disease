{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load and initialize required libraries"
      ],
      "metadata": {
        "id": "LxkPyZV4g3Fj"
      },
      "id": "LxkPyZV4g3Fj"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kfp"
      ],
      "metadata": {
        "id": "zmXV5HQqnf7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666582072,
          "user_tz": 420,
          "elapsed": 11183,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "39ae7bee-a884-438a-ec71-2af2cdf8598e"
      },
      "id": "zmXV5HQqnf7P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kfp in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.7)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.16)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.8.0)\n",
            "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.3.0)\n",
            "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (2.0.5)\n",
            "Requirement already satisfied: kubernetes<27,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (26.1.0)\n",
            "Requirement already satisfied: protobuf<5,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from kfp) (4.25.3)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.10.1)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.26.18)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.63.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.2.1->kfp) (2.7.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (67.7.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3,>=2.2.1->kfp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-pipeline-components"
      ],
      "metadata": {
        "id": "aG3wxHPGpcJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666593245,
          "user_tz": 420,
          "elapsed": 11184,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1f14bde3-a0f1-49ec-fca1-09260e4dcf90"
      },
      "id": "aG3wxHPGpcJw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-pipeline-components in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (2.11.1)\n",
            "Requirement already satisfied: kfp<=2.7.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (2.7.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (1.47.0)\n",
            "Requirement already satisfied: Jinja2<4,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pipeline-components) (3.1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (4.25.3)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.27.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (2.31.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (24.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.12.3)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.0.3)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.10.15)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (2.1.5)\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (8.1.7)\n",
            "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (0.3.0)\n",
            "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (2.0.5)\n",
            "Requirement already satisfied: kubernetes<27,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (26.1.0)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (0.10.1)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.26.18)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.7.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (0.13.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (2024.2.2)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (67.7.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (3.7)\n",
            "Requirement already satisfied: numpy<2,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.25.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp<=2.7.0,>=2.6.0->google-cloud-pipeline-components) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs"
      ],
      "metadata": {
        "id": "84u2hUWp63hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666603586,
          "user_tz": 420,
          "elapsed": 10367,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fe1c9a01-73c4-41f1-ba97-6009232c5fba"
      },
      "id": "84u2hUWp63hh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (3.9.5)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2023.6.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.31.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2024.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (4.25.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "vyQIEz4jqWqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666613633,
          "user_tz": 420,
          "elapsed": 10091,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f399da82-ad67-4e7f-d0d2-1f4550de7ab4"
      },
      "id": "vyQIEz4jqWqk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Set parameters and initialize aiplatform client library"
      ],
      "metadata": {
        "id": "iWyCC-sd-NX2"
      },
      "id": "iWyCC-sd-NX2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "project_id = 'ise543-module7-homework-418819'\n",
        "location = 'us-central1'"
      ],
      "metadata": {
        "id": "0is-1ZChh7TU"
      },
      "id": "0is-1ZChh7TU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=project_id, location=location)\n",
        "\n",
        "from kfp.v2.dsl import pipeline, component"
      ],
      "metadata": {
        "id": "POu_bl7VBqLp",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666619753,
          "user_tz": 420,
          "elapsed": 6152,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090c23c4-a6a6-43df-a9c1-ec284c8ab00e"
      },
      "id": "POu_bl7VBqLp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-ba0b4819e567>:4: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
            "  from kfp.v2.dsl import pipeline, component\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define components"
      ],
      "metadata": {
        "id": "Rw4wV0aWAbYG"
      },
      "id": "Rw4wV0aWAbYG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common dataset preparation steps"
      ],
      "metadata": {
        "id": "n8RlAETQjqRA"
      },
      "id": "n8RlAETQjqRA"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import InputPath, OutputPath, Dataset\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"fsspec\", \"gcsfs\"])\n",
        "def perform_initial_data_preparation(input_dataset_path: str, output_dataset_path: OutputPath(Dataset)):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = pd.read_csv(input_dataset_path)\n",
        "\n",
        "    df['income_log'] = np.log1p(df['income'])\n",
        "    df.drop(['income', 'glucose'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "    df['education'].fillna(5, inplace=True)\n",
        "\n",
        "\n",
        "    df.to_csv(output_dataset_path, index=False)\n"
      ],
      "metadata": {
        "id": "_4OiYx7sj6Nd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666619754,
          "user_tz": 420,
          "elapsed": 44,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a781243-040e-4a01-c061-420c27ab779c"
      },
      "id": "_4OiYx7sj6Nd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
            "  return component_factory.create_component_from_func(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One-Hot encoding"
      ],
      "metadata": {
        "id": "0Srq6YElt82k"
      },
      "id": "0Srq6YElt82k"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import InputPath\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
        "def onehot_encoding(dataset_path: InputPath('Dataset'),\n",
        "                  output_path: OutputPath('Dataset')\n",
        "                  ):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df_education = pd.get_dummies(df['education'],drop_first=True)\n",
        "    df_education.columns = ['education_2', 'education_3', 'education_4', 'education_5']\n",
        "    df_education = df_education.astype(int)\n",
        "\n",
        "    df = pd.concat([df, df_education], axis=1)\n",
        "    df.to_csv(output_path, index=False )"
      ],
      "metadata": {
        "id": "27ybvmDvuAmL"
      },
      "id": "27ybvmDvuAmL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train-Test Split"
      ],
      "metadata": {
        "id": "BLO2FUuTufaE"
      },
      "id": "BLO2FUuTufaE"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import InputPath\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
        "def split_dataset(dataset_path: InputPath('Dataset'),\n",
        "                  training_dataset_path: OutputPath('Dataset'),\n",
        "                  validation_dataset_path: OutputPath('Dataset')):\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    train_df, val_df = train_test_split(df, test_size=0.20, random_state=42)\n",
        "    train_df.to_csv(training_dataset_path, index=False)\n",
        "    val_df.to_csv(validation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "k5emW-6Pa31g"
      },
      "id": "k5emW-6Pa31g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outlier Handling Training Component"
      ],
      "metadata": {
        "id": "jt5yJS2DIaRi"
      },
      "id": "jt5yJS2DIaRi"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import InputPath, Output, Artifact\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"numpy\"])\n",
        "def outlier_training(training_dataset_path: InputPath('Dataset'),\n",
        "                  training_outlier_output_path: OutputPath('Dataset'),\n",
        "                  iqr_values: Output[Artifact]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    columns = df.columns.tolist()\n",
        "\n",
        "    Q1 = df.quantile(0.25)\n",
        "    Q3 = df.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    for col in df.columns:\n",
        "      if col in ['a1c', 'cigsPerDay', 'income_log', 'totChol', 'sysBP',  'BMI']:\n",
        "        df[col] = np.where(df[col] < lower_bound[col], lower_bound[col], df[col])\n",
        "        df[col] = np.where(df[col] > upper_bound[col], upper_bound[col], df[col])\n",
        "\n",
        "    df.to_csv(training_outlier_output_path, index=False)\n",
        "    # Output the IQR values\n",
        "    iqr_values.metadata['columns'] = ', '.join(df.columns.tolist())\n",
        "    iqr_values.metadata['Q1'] = Q1.to_dict()\n",
        "    iqr_values.metadata['Q3'] = Q3.to_dict()\n",
        "    iqr_values.metadata['IQR'] = IQR.to_dict()"
      ],
      "metadata": {
        "id": "rLF81ax8IgaV"
      },
      "id": "rLF81ax8IgaV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Outlier Handling Validation component"
      ],
      "metadata": {
        "id": "erDL6zw4RZr4"
      },
      "id": "erDL6zw4RZr4"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Input\n",
        "from kfp.v2.dsl import Model\n",
        "\n",
        "@component(packages_to_install=[\"pandas\"])\n",
        "def outlier_validation(validation_dataset_path: InputPath('Dataset'),\n",
        "                      validation_outlier_output_path: OutputPath('Dataset'),\n",
        "                      iqr_values: Input[Artifact]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    # Load the test dataset\n",
        "    df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "\n",
        "    # Access the IQR values from the artifact metadata\n",
        "    iqr_metadata = iqr_values.metadata\n",
        "    Q1 = pd.Series(iqr_metadata['Q1'])\n",
        "    Q3 = pd.Series(iqr_metadata['Q3'])\n",
        "    IQR = pd.Series(iqr_metadata['IQR'])\n",
        "\n",
        "    # Apply outlier detection based on the IQR values\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    for col in df.columns:\n",
        "        if col in ['a1c', 'cigsPerDay', 'income_log', 'totChol', 'sysBP',  'BMI']:\n",
        "          df[col] = np.where(df[col] < lower_bound[col], lower_bound[col], df[col])\n",
        "          df[col] = np.where(df[col] > upper_bound[col], upper_bound[col], df[col])\n",
        "\n",
        "\n",
        "    # Save the imputed dataframe to the output path\n",
        "    df.to_csv(validation_outlier_output_path, index=False)"
      ],
      "metadata": {
        "id": "c-OfKN1FRfpg"
      },
      "id": "c-OfKN1FRfpg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impute training component"
      ],
      "metadata": {
        "id": "_-enciF4AwPJ"
      },
      "id": "_-enciF4AwPJ"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from kfp.v2.dsl import Output\n",
        "from kfp.v2.dsl import Artifact\n",
        "\n",
        "@component(packages_to_install=[\"pandas\"])\n",
        "def impute_training(training_dataset_path: InputPath('Dataset'),\n",
        "                   imputed_dataset_path: OutputPath('Dataset'),\n",
        "                   imputed_values: Output[Artifact]):\n",
        "    # Load the training dataset\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    # Columns to impute median values for\n",
        "    columns_to_impute = ['cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'a1c']\n",
        "\n",
        "    # Calculate and store median values for specified columns\n",
        "    median_values = df[columns_to_impute].median()\n",
        "\n",
        "    # Perform imputation for each column\n",
        "    for col in columns_to_impute:\n",
        "        df[col].fillna(median_values[col], inplace=True)\n",
        "\n",
        "    # Save the imputed dataframe to the output path\n",
        "    df.to_csv(imputed_dataset_path, index=False)\n",
        "\n",
        "    # Output the median values\n",
        "    imputed_values.metadata['columns'] = ', '.join(columns_to_impute)\n",
        "    imputed_values.metadata['medians'] = median_values.to_dict()"
      ],
      "metadata": {
        "id": "ldZe4MVEeRgR"
      },
      "id": "ldZe4MVEeRgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Impute validation component"
      ],
      "metadata": {
        "id": "GfVG15wuA1nP"
      },
      "id": "GfVG15wuA1nP"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Input\n",
        "from kfp.v2.dsl import Model\n",
        "\n",
        "@component(packages_to_install=[\"pandas\"])\n",
        "def impute_validation(validation_dataset_path: InputPath('Dataset'),\n",
        "                      imputed_dataset_path: OutputPath('Dataset'),\n",
        "                      imputed_values: Input[Artifact]):\n",
        "    import pandas as pd\n",
        "    # Load the test dataset\n",
        "    df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    # Columns to impute median values for\n",
        "    columns_to_impute = ['cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'a1c']\n",
        "\n",
        "    # Impute missing values using the provided median values\n",
        "    for col in columns_to_impute:\n",
        "        df[col].fillna(imputed_values.metadata['medians'][col], inplace=True)\n",
        "\n",
        "    # Save the imputed dataframe to the output path\n",
        "    df.to_csv(imputed_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "dU1V7mjNg-07"
      },
      "id": "dU1V7mjNg-07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalise Training"
      ],
      "metadata": {
        "id": "91GSZfOTnROr"
      },
      "id": "91GSZfOTnROr"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Output\n",
        "from kfp.v2.dsl import Artifact\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def normalise_training(\n",
        "    training_dataset_path: InputPath('Dataset'),\n",
        "    normalised_training_dataset_path: OutputPath('Dataset'),\n",
        "    scaler_path: Output[Artifact]\n",
        "):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "\n",
        "    # Load the training dataset\n",
        "    training_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    # Define the columns to scale\n",
        "    columns_to_scale = ['age',  'cigsPerDay', 'totChol', 'sysBP',  'BMI', 'heartRate',  'a1c', 'income_log', 'diaBP']\n",
        "\n",
        "    # Separate the columns to be scaled\n",
        "    X_train_scaled = training_df[columns_to_scale]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the columns to be scaled\n",
        "    X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
        "\n",
        "    training_df[columns_to_scale] = X_train_scaled\n",
        "\n",
        "\n",
        "    # Save the normalised training DataFrame to the output path\n",
        "    training_df.to_csv(normalised_training_dataset_path, index=False)\n",
        "\n",
        "    # Save the scaler to the output path\n",
        "    joblib.dump(scaler, scaler_path.path)\n"
      ],
      "metadata": {
        "id": "zvxrvRhdnP0U"
      },
      "id": "zvxrvRhdnP0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalise Validation"
      ],
      "metadata": {
        "id": "sYqXb97SRITI"
      },
      "id": "sYqXb97SRITI"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Input\n",
        "from kfp.v2.dsl import Artifact\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\",\"joblib\"])\n",
        "def normalise_validation(\n",
        "                       validation_dataset_path: InputPath('Dataset'),\n",
        "                       scaler_path: Input[Artifact],\n",
        "                      normalised_validation_dataset_path: OutputPath('Dataset')\n",
        "                       ):\n",
        "\n",
        "    # Load the training dataset\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "\n",
        "    scaler = joblib.load(scaler_path.path)\n",
        "\n",
        "    validation_df = pd.read_csv(validation_dataset_path)\n",
        "\n",
        "    # Define the columns to scale\n",
        "    columns_to_scale = ['age',  'cigsPerDay', 'totChol', 'sysBP',  'BMI', 'heartRate',  'a1c', 'income_log', 'diaBP']\n",
        "\n",
        "    # Separate the columns to be scaled\n",
        "    X_val_scaled = validation_df[columns_to_scale]\n",
        "\n",
        "\n",
        "    X_validation_normalized = scaler.transform(X_val_scaled)\n",
        "\n",
        "    validation_df[columns_to_scale] = X_validation_normalized\n",
        "\n",
        "    # Save the imputed dataframe to the output path\n",
        "    validation_df.to_csv(normalised_validation_dataset_path, index=False)"
      ],
      "metadata": {
        "id": "f6aXX4kARM_B"
      },
      "id": "f6aXX4kARM_B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Perform SMOTE oversampling on training partition"
      ],
      "metadata": {
        "id": "en9dbR3iZJgx"
      },
      "id": "en9dbR3iZJgx"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\", \"scikit-learn\", \"imbalanced-learn==0.11.0\"])\n",
        "def perform_SMOTE(imputed_training_path:  InputPath('Dataset'),\n",
        "                  smote_output_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    # Load the input dataset\n",
        "    df = pd.read_csv(imputed_training_path)\n",
        "\n",
        "\n",
        "    X = df.drop('TenYearCHD', axis = 1)\n",
        "    y = df['TenYearCHD']\n",
        "\n",
        "    # Perform SMOTE oversampling\n",
        "    smote = SMOTE(random_state=42)  # random_state=42, sampling_strategy=0.5)\n",
        "    # smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
        "\n",
        "\n",
        "    X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "    # Convert the oversampled feature set and target vector back into a DataFrame\n",
        "    X_smote_df = pd.DataFrame(X_smote, columns=X.columns)\n",
        "    y_smote_df = pd.DataFrame(y_smote, columns=['TenYearCHD'])\n",
        "\n",
        "    # Re-join the features and the target into a single DataFrame\n",
        "    oversampled_df = pd.concat([X_smote_df, y_smote_df], axis=1)\n",
        "\n",
        "    # Save the re-joined, oversampled dataset to the specified OutputPath\n",
        "    oversampled_df.to_csv(smote_output_path, index=False)"
      ],
      "metadata": {
        "id": "8tvGx1ixZNro"
      },
      "id": "8tvGx1ixZNro",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Train a model"
      ],
      "metadata": {
        "id": "tkZBvr6Yf-xe"
      },
      "id": "tkZBvr6Yf-xe"
    },
    {
      "cell_type": "code",
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_model(training_dataset_path: InputPath('Dataset'),\n",
        "                              trained_model_artifact: OutputPath('Model')):\n",
        "\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    # from sklearn.linear_model import LogisticRegression\n",
        "    import joblib\n",
        "    import os\n",
        "\n",
        "    # Load the training data\n",
        "    train_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "    X_train = train_df.drop('TenYearCHD', axis=1)\n",
        "    y_train = train_df['TenYearCHD']\n",
        "\n",
        "    trained_model = RandomForestClassifier(random_state=42)\n",
        "    trained_model.fit(X_train, y_train)\n",
        "\n",
        "    joblib.dump(trained_model, trained_model_artifact)\n",
        "    # Save the model to the designated gcs output path\n",
        "    # os.makedirs(trained_model_artifact.path, exist_ok=True)\n",
        "    # joblib.dump(trained_model, os.path.join(trained_model_artifact.path, \"model.joblib\"))"
      ],
      "metadata": {
        "id": "zOtbYmlnHRnU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zOtbYmlnHRnU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Evaluate model"
      ],
      "metadata": {
        "id": "jk_WfRjbgQsH"
      },
      "id": "jk_WfRjbgQsH"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import Metrics\n",
        "\n",
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\", \"xgboost==1.6.2\" ])\n",
        "def evaluate_model(test_dataset_path: InputPath('Dataset'),\n",
        "                   model: InputPath('Model'),\n",
        "                   metrics: Output[Metrics]):\n",
        "\n",
        "    import pandas as pd\n",
        "    import joblib\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, average_precision_score\n",
        "\n",
        "    # Load the test dataset\n",
        "    test_df = pd.read_csv(test_dataset_path)\n",
        "    X_test = test_df.drop(columns=['TenYearCHD'])\n",
        "    y_test = test_df['TenYearCHD']\n",
        "\n",
        "    # Load the trained model\n",
        "    trained_model = joblib.load(model)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = trained_model.predict(X_test)\n",
        "\n",
        "    # Calculate the confusion matrix and extract components\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    auc_prc = average_precision_score(y_test, y_pred)\n",
        "\n",
        "    # Log each component of the confusion matrix separately\n",
        "    metrics.log_metric(\"accuracy\", accuracy)\n",
        "    metrics.log_metric(\"f1_score\", f1)\n",
        "    metrics.log_metric(\"true_negatives\", int(tn))\n",
        "    metrics.log_metric(\"false_positives\", int(fp))\n",
        "    metrics.log_metric(\"false_negatives\", int(fn))\n",
        "    metrics.log_metric(\"true_positives\", int(tp))\n",
        "    metrics.log_metric(\"auc_prc\", auc_prc)\n",
        "\n"
      ],
      "metadata": {
        "id": "aMA_2eIGYZZF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aMA_2eIGYZZF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define pipeline"
      ],
      "metadata": {
        "id": "RoIaZjDUBI4B"
      },
      "id": "RoIaZjDUBI4B"
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2.dsl import pipeline, Output, Dataset\n",
        "\n",
        "@pipeline(name='fp-pipeline')\n",
        "def fp_pipeline(training_dataset_path:str):\n",
        "\n",
        "    # Process training dataset - initial data preparation\n",
        "    training_data_preparation = perform_initial_data_preparation(input_dataset_path=training_dataset_path)\n",
        "\n",
        "    #Process One-hot encoding\n",
        "    one_hot_encoding = onehot_encoding(dataset_path=training_data_preparation.outputs['output_dataset_path'])\n",
        "\n",
        "    # Process Split\n",
        "    data_split = split_dataset(dataset_path=one_hot_encoding.outputs['output_path'])\n",
        "\n",
        "    #Outlier training set\n",
        "    outlier_training_result = outlier_training(training_dataset_path=data_split.outputs['training_dataset_path'])\n",
        "\n",
        "    #Outlier validation set\n",
        "    outlier_validation_result = outlier_validation(validation_dataset_path=data_split.outputs['validation_dataset_path'],\n",
        "                                                   iqr_values=outlier_training_result.outputs['iqr_values'])\n",
        "\n",
        "    # Impute training dataset\n",
        "    training_data = impute_training(training_dataset_path=outlier_training_result.outputs['training_outlier_output_path'])\n",
        "\n",
        "    # Impute validation dataset\n",
        "    validation_data = impute_validation(validation_dataset_path=outlier_validation_result.outputs['validation_outlier_output_path'],\n",
        "                                        imputed_values=training_data.outputs['imputed_values'] )\n",
        "\n",
        "    #Normalise training set\n",
        "    normalised_training_result = normalise_training(training_dataset_path=training_data.outputs['imputed_dataset_path'])\n",
        "\n",
        "    #Normalise validaton set\n",
        "    normalised_validation_result = normalise_validation(validation_dataset_path=validation_data.outputs['imputed_dataset_path'],\n",
        "                                                        scaler_path=normalised_training_result.outputs['scaler_path'])\n",
        "\n",
        "\n",
        "    # Perform SMOTE oversampling on the training partition\n",
        "    oversampled_training_data = perform_SMOTE(imputed_training_path=normalised_training_result.outputs['normalised_training_dataset_path'])\n",
        "\n",
        "    # Train a Random Forest model\n",
        "    trained_model =  train_model(training_dataset_path=oversampled_training_data.outputs['smote_output_path'])\n",
        "\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(\n",
        "        test_dataset_path=normalised_validation_result.outputs['normalised_validation_dataset_path'],\n",
        "        model=trained_model.outputs['trained_model_artifact']\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7rMmQYvcoaRl"
      },
      "id": "7rMmQYvcoaRl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Compile and run pipeline"
      ],
      "metadata": {
        "id": "GtRqnVbnBM-b"
      },
      "id": "GtRqnVbnBM-b"
    },
    {
      "cell_type": "code",
      "source": [
        "REGION = 'us-central1'\n",
        "BUCKET_URI = \"gs://finalproject_ise543\""
      ],
      "metadata": {
        "id": "0pvr1FBqYmNm"
      },
      "id": "0pvr1FBqYmNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SERVICE_ACCOUNT = \"753516815850-compute@developer.gserviceaccount.com\""
      ],
      "metadata": {
        "id": "fgpJpdgWYuFX"
      },
      "id": "fgpJpdgWYuFX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil iam ch serviceAccount : {SERVICE_ACCOUNT}: roles/storage.objectCreator $BUCKET_URI\n",
        "!gsutil iam ch serviceAccount : {SERVICE_ACCOUNT}: roles/storage.objectViewer $BUCKET_URI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRa3BoFRY6hN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666628638,
          "user_tz": 420,
          "elapsed": 8649,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c3f9576b-6f91-4272-9814-8776346b3169"
      },
      "id": "xRa3BoFRY6hN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CommandException: Must specify a role to grant.\n",
            "CommandException: Must specify a role to grant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2 import compiler\n",
        "\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=fp_pipeline,\n",
        "    package_path = 'fp_pipeline.json'\n",
        ")\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='fp_modeling_pipeline',\n",
        "    template_path='fp_pipeline.json',\n",
        "    pipeline_root='gs://finalproject_ise543',\n",
        "    parameter_values={\n",
        "      'training_dataset_path': 'gs://finalproject_ise543/Final Project Dataset.csv'\n",
        "    },\n",
        "    enable_caching=True\n",
        ")\n",
        "\n",
        "pipeline_job.run()"
      ],
      "metadata": {
        "id": "0U3yrSwInxSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff194887-da1d-4440-c489-5be26f779de7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714666640920,
          "user_tz": 420,
          "elapsed": 12299,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "0U3yrSwInxSo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/753516815850/locations/us-central1/pipelineJobs/fp-pipeline-20240502161707\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/753516815850/locations/us-central1/pipelineJobs/fp-pipeline-20240502161707')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fp-pipeline-20240502161707?project=753516815850\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/753516815850/locations/us-central1/pipelineJobs/fp-pipeline-20240502161707 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/753516815850/locations/us-central1/pipelineJobs/fp-pipeline-20240502161707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# path = \"gs://finalproject_ise543/753516815850/fp-pipeline-20240420210851/normalise-training_8827496792284200960/normalised_training_dataset_path\"\n",
        "# df = pd.read_csv(path)\n",
        "# df.info()"
      ],
      "metadata": {
        "id": "SexMNrhpW7hC"
      },
      "id": "SexMNrhpW7hC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "SjzSKWbbXLFZ"
      },
      "id": "SjzSKWbbXLFZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "fp",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}